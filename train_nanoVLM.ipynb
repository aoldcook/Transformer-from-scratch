{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2de5dd1f",
      "metadata": {
        "id": "2de5dd1f"
      },
      "source": [
        "### Train a VLM in Google Colab!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OCooV08mNANR",
      "metadata": {
        "id": "OCooV08mNANR"
      },
      "source": [
        "### Clone the repository if you don't have it already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ooQMjmrMLn-4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooQMjmrMLn-4",
        "outputId": "29ab4889-11c3-4eed-db2e-18c399d3f7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoVLM'...\n",
            "remote: Enumerating objects: 1679, done.\u001b[K\n",
            "remote: Counting objects: 100% (1211/1211), done.\u001b[K\n",
            "remote: Compressing objects: 100% (376/376), done.\u001b[K\n",
            "remote: Total 1679 (delta 900), reused 835 (delta 835), pack-reused 468 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1679/1679), 13.45 MiB | 18.43 MiB/s, done.\n",
            "Resolving deltas: 100% (1166/1166), done.\n",
            "/content/nanoVLM\n",
            "assets\t    evaluation.py  prepare.sh\t      tests\t\t   utils\n",
            "data\t    generate.py    README.md\t      train_nanoVLM.ipynb\n",
            "eval\t    LICENSE\t   run_evaluation.py  train.py\n",
            "eval.slurm  models\t   slurm\t      train.sh\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('nanoVLM'):\n",
        "    !git clone https://github.com/huggingface/nanoVLM.git\n",
        "%cd nanoVLM/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mMhc9OCENup5",
      "metadata": {
        "id": "mMhc9OCENup5"
      },
      "source": [
        "### Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bcw8qQqoOSR7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcw8qQqoOSR7",
        "outputId": "ff027bb1-6a72-4b3a-fb4c-8ce0b57e54fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m481.3/491.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# If you get an \"Error\" from pip's dependency resolver but the cell completes fine, this is not an issue, you can continue :)\n",
        "!pip -q install torch\n",
        "!pip -q install gcsfs\n",
        "!pip -q install datasets==3.5.0\n",
        "!pip -q install tqdm\n",
        "!pip -q install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "54bc8463",
      "metadata": {
        "id": "54bc8463",
        "outputId": "e33578dc-4fb1-4024-bbf0-cd565ddb5cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "ed4047c923264b84b287185cb20e1be5",
            "20191db82876455caa2f602bc1f203ed",
            "0d8b5fe85096456b805c115b18d51fb5",
            "cb61f8ac0ddb4411a733b95c04609c4f",
            "a2f93091173d421dbb3e752c21270b45",
            "a74ca7bb1a5444e7982deeffc1e2ab12",
            "b3fb8d676b4f41c7bebe86404bf1de96",
            "96f0cd89f26f42efa190035c3a5c4099",
            "0858278bb548452384b6522759dd0728",
            "7cd1449fa16c462e96fa8fdbc4c94c49",
            "b26e5515e862490ea9b2a4a35015a86d",
            "4ed4d2e22908481ab7db4f1e14ded317",
            "41198e8b25f54b93ae0dadb07abb5130",
            "92ffc2d0fbc14caaaee083607901b5e6",
            "6c4868f079dd4960a41a06b695d0fbcc",
            "2ec722d1e43e473f8167df2eed9f66ed",
            "aee7f4640b7b4ef9a657ca83c542e4fc",
            "c90bc10cb3a6462f98d2a141e8685a1b",
            "4e504b4d2ca24bb497905a7c4c23145d",
            "568e8646239d4b3693b88f427ee1df8a"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed4047c923264b84b287185cb20e1be5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let's authentificate with the Hugging Face Hub so you can push your model\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5e8dc5ba",
      "metadata": {
        "id": "5e8dc5ba"
      },
      "outputs": [],
      "source": [
        "# Decide on the name of your model here!\n",
        "# You will need your HF user name and the name you want to give to it\n",
        "# For me, this would be \"lusxvr/nanoVLM\"\n",
        "hf_model_name = \"YOUR-HF-USERNAME/nanoVLM\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip  install imp"
      ],
      "metadata": {
        "id": "TQMt1JbhOTOJ",
        "outputId": "d740428d-d96e-44aa-962a-ed8532cd61e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TQMt1JbhOTOJ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement imp (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for imp\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "OTsl1jZrMeaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTsl1jZrMeaJ",
        "outputId": "3408fb71-3ac9-48f3-e66d-0b00a1e3d9d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# nanoVLM Imports (please check out the implementations in detail, that's where all the interesting stuff is!)\n",
        "from data.datasets import VQADataset\n",
        "from data.collators import VQACollator\n",
        "from data.data_utils import synchronized_dataloader_step\n",
        "from data.advanced_datasets import ConstantLengthDataset\n",
        "from data.processors import get_image_processor, get_tokenizer\n",
        "\n",
        "import models.config as config\n",
        "from models.vision_language_model import VisionLanguageModel\n",
        "\n",
        "# Libraries\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass, field\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, concatenate_datasets, get_dataset_config_names\n",
        "\n",
        "#Otherwise, the tokenizer will through a warning\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# To reload the modules if you change something in the code\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Vzo03IzN3Zf",
      "metadata": {
        "id": "4Vzo03IzN3Zf"
      },
      "source": [
        "### Get the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3Zzn2FI2N7Aj",
      "metadata": {
        "id": "3Zzn2FI2N7Aj"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(train_cfg, vlm_cfg):\n",
        "    # Create datasets\n",
        "    image_processor = get_image_processor(vlm_cfg.max_img_size, vlm_cfg.vit_img_size, vlm_cfg.resize_to_max_side_len)\n",
        "    tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer, vlm_cfg.vlm_extra_tokens, vlm_cfg.lm_chat_template)\n",
        "\n",
        "    # Load and combine all training datasets\n",
        "    dataset_names_to_load = train_cfg.train_dataset_name\n",
        "    if \"all\" in dataset_names_to_load:\n",
        "        dataset_names_to_load = get_dataset_config_names(train_cfg.train_dataset_path)\n",
        "\n",
        "    combined_train_data = []\n",
        "\n",
        "    for dataset_name in dataset_names_to_load:\n",
        "        print(f\"Loading dataset: {dataset_name}\")\n",
        "        try:\n",
        "            train_ds = load_dataset(train_cfg.train_dataset_path, dataset_name)['train']\n",
        "            train_ds[0] # Check if the dataset is loaded correctly\n",
        "            combined_train_data.append(train_ds)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to load dataset config '{dataset_name}' from '{train_cfg.train_dataset_path}'. Error: {e}\")\n",
        "            continue\n",
        "    train_ds = concatenate_datasets(combined_train_data)\n",
        "\n",
        "    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatenated datasets\n",
        "\n",
        "    # Apply cutoff if specified\n",
        "    if train_cfg.data_cutoff_idx is None:\n",
        "        total_samples = len(train_ds)  # Use the entire dataset\n",
        "    else:\n",
        "        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n",
        "\n",
        "    val_size = int(total_samples * train_cfg.val_ratio)\n",
        "    train_size = total_samples - val_size\n",
        "\n",
        "    val_ds = train_ds.select(range(train_size, total_samples-1))\n",
        "    train_ds = train_ds.select(range(train_size))\n",
        "\n",
        "    train_dataset = VQADataset(train_ds, tokenizer, image_processor, vlm_cfg.mp_image_token_length)\n",
        "    val_dataset = VQADataset(val_ds, tokenizer, image_processor, vlm_cfg.mp_image_token_length)\n",
        "\n",
        "    train_dataset = ConstantLengthDataset(train_dataset, infinite=False, max_sample_length=train_cfg.max_sample_length, seq_length=vlm_cfg.lm_max_length, num_of_sequences=train_cfg.batch_size*4, queue_size=8,\n",
        "                                        max_images_per_example=train_cfg.max_images_per_example, max_images_per_knapsack=train_cfg.max_images_per_knapsack)\n",
        "\n",
        "    # Create collators\n",
        "    vqa_collator = VQACollator(tokenizer, vlm_cfg.lm_max_length)\n",
        "\n",
        "    # Create dataloaders\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_cfg.batch_size,    # =per device BS in DDP\n",
        "        collate_fn=vqa_collator,\n",
        "        num_workers=1,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=vqa_collator,\n",
        "        num_workers=1,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    # Warmup dataloaders to kickstart worker processes\n",
        "    print(\"Warming up dataloaders...\")\n",
        "    next(iter(train_loader))\n",
        "    next(iter(val_loader))\n",
        "    print(\"Warmup complete.\")\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_F8u3MJ6PAfd",
      "metadata": {
        "id": "_F8u3MJ6PAfd"
      },
      "source": [
        "### Prepare the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "KxOtMU5zPD-4",
      "metadata": {
        "id": "KxOtMU5zPD-4"
      },
      "outputs": [],
      "source": [
        "def get_lr(it, max_lr, max_steps):\n",
        "    min_lr = max_lr * 0.1\n",
        "    warmup_steps = max_steps * 0.03\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "def train(train_cfg, vlm_cfg):\n",
        "    train_loader, val_loader = get_dataloaders(train_cfg, vlm_cfg)\n",
        "\n",
        "    # Initialize model\n",
        "    if train_cfg.resume_from_vlm_checkpoint:\n",
        "        print(f\"Resuming from VLM checkpoint: {vlm_cfg.vlm_checkpoint_path}\")\n",
        "        model = VisionLanguageModel.from_pretrained(vlm_cfg.vlm_checkpoint_path)\n",
        "    else:\n",
        "        model = VisionLanguageModel(vlm_cfg, load_backbone=vlm_cfg.vlm_load_backbone_weights)\n",
        "\n",
        "    print(f\"nanoVLM initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "    print(f\"Training summary: {len(train_loader.dataset)} samples, {len(train_loader)} batches/epoch, batch size {train_cfg.batch_size}\")\n",
        "\n",
        "    # Define optimizer groups\n",
        "    # Since we have pretrained vision and language backbones, but a newly initialized modality projection layer, it doesn't make sense to train them with the same learning rate\n",
        "    # You could opt to fully freeze the backbones and only train the MP layer, but finetuning them with a lower learning rate makes the training as a whole easier\n",
        "    param_groups = []\n",
        "    if train_cfg.lr_mp > 0:\n",
        "        param_groups.append({'params': list(model.MP.parameters()), 'lr': train_cfg.lr_mp})\n",
        "    else:\n",
        "        for p in list(model.MP.parameters()):\n",
        "            p.requires_grad = False\n",
        "    if train_cfg.lr_vision_backbone > 0:\n",
        "        param_groups.append({'params': list(model.vision_encoder.parameters()), 'lr': train_cfg.lr_vision_backbone})\n",
        "    else:\n",
        "        for p in list(model.vision_encoder.parameters()):\n",
        "            p.requires_grad = False\n",
        "    if train_cfg.lr_language_backbone > 0:\n",
        "        param_groups.append({'params': list(model.decoder.parameters()), 'lr': train_cfg.lr_language_backbone})\n",
        "    else:\n",
        "        for p in list(model.decoder.parameters()):\n",
        "            p.requires_grad = False\n",
        "\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "    all_params = [p for group in optimizer.param_groups for p in group['params']]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    epoch_times = []\n",
        "    batch_losses = []\n",
        "    val_losses = []\n",
        "    val_plot_steps = []\n",
        "    global_step = 0\n",
        "    epoch = 0\n",
        "\n",
        "    while global_step < train_cfg.max_training_steps:\n",
        "        epoch_start_time = time.time()\n",
        "        epoch += 1\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_tokens_processed = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(\"Starting training loop\")\n",
        "        for i, batch in enumerate(synchronized_dataloader_step(train_loader, False)):\n",
        "            batch_start_time = time.time()\n",
        "            is_update_step = (i + 1) % train_cfg.gradient_accumulation_steps == 0 or i + 1 == len(train_loader)\n",
        "            images = batch[\"images\"]\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16): # Mixed precision training\n",
        "                _, loss = model(input_ids, images, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "            if train_cfg.gradient_accumulation_steps > 1:\n",
        "                loss = loss / train_cfg.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if is_update_step:\n",
        "                if train_cfg.max_grad_norm is not None:\n",
        "                    _ = torch.nn.utils.clip_grad_norm_(all_params, max_norm=train_cfg.max_grad_norm)\n",
        "\n",
        "                param_group_idx = 0\n",
        "                if train_cfg.lr_mp > 0:\n",
        "                    adj_lr_mp = get_lr(global_step, train_cfg.lr_mp, train_cfg.max_training_steps)\n",
        "                    optimizer.param_groups[param_group_idx]['lr'] = adj_lr_mp\n",
        "                    param_group_idx += 1\n",
        "\n",
        "                if train_cfg.lr_vision_backbone > 0:\n",
        "                    adj_lr_vision_backbone = get_lr(global_step, train_cfg.lr_vision_backbone, train_cfg.max_training_steps)\n",
        "                    optimizer.param_groups[param_group_idx]['lr'] = adj_lr_vision_backbone\n",
        "                    param_group_idx += 1\n",
        "\n",
        "                if train_cfg.lr_language_backbone > 0:\n",
        "                    adj_lr_language_backbone = get_lr(global_step, train_cfg.lr_language_backbone, train_cfg.max_training_steps)\n",
        "                    optimizer.param_groups[param_group_idx]['lr'] = adj_lr_language_backbone\n",
        "\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            if train_cfg.gradient_accumulation_steps > 1:\n",
        "                batch_loss = batch_loss * train_cfg.gradient_accumulation_steps\n",
        "            total_train_loss += batch_loss\n",
        "            batch_losses.append(batch_loss)\n",
        "\n",
        "            num_tokens = torch.sum(attention_mask).item() # Sum of attention mask gives number of tokens\n",
        "            total_tokens_processed += num_tokens\n",
        "\n",
        "            batch_end_time = time.time()\n",
        "            batch_duration = batch_end_time - batch_start_time\n",
        "            tokens_per_second = num_tokens / batch_duration\n",
        "\n",
        "            if global_step % 20 == 0:\n",
        "                model.eval()\n",
        "                torch.cuda.empty_cache()  # Clear GPU memory\n",
        "                with torch.no_grad():\n",
        "                    total_val_loss = 0\n",
        "                    for batch in synchronized_dataloader_step(val_loader, False):\n",
        "                        images = batch[\"images\"]\n",
        "                        input_ids = batch[\"input_ids\"].to(device)\n",
        "                        labels = batch[\"labels\"].to(device)\n",
        "                        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "                        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                            _, loss = model(input_ids, images, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "                        total_val_loss += loss.item()\n",
        "                    avg_val_loss = total_val_loss / len(val_loader)\n",
        "                    val_losses.append(avg_val_loss)\n",
        "                    val_plot_steps.append(global_step)\n",
        "                print(f\"\\nStep: {global_step}, Loss: {batch_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Tokens/s: {tokens_per_second:.2f}\")\n",
        "                model.train()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        epoch_times.append(epoch_duration)\n",
        "\n",
        "        epoch_tokens_per_second = total_tokens_processed / epoch_duration\n",
        "\n",
        "        print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {epoch_duration:.2f}s | T/s: {epoch_tokens_per_second:.2f}\")\n",
        "\n",
        "    model.save_pretrained(save_directory=vlm_cfg.vlm_checkpoint_path)\n",
        "    model.push_to_hub(hf_model_name)\n",
        "\n",
        "    total_training_time = sum(epoch_times)\n",
        "    print(f\"Total training time: {total_training_time:.2f}s\")\n",
        "\n",
        "    plt.plot(batch_losses, label='Train Loss')\n",
        "    plt.plot(val_plot_steps, val_losses, label='Val Loss')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4HmsEPNQZbh",
      "metadata": {
        "id": "d4HmsEPNQZbh"
      },
      "source": [
        "### Prepare the Configs\n",
        "Instead of using the config.py file in the repo (which was created to run on one H100), we will create our config here to play around with the parameters easier and adapt them to colabs capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "h8FlqtizQdO-",
      "metadata": {
        "id": "h8FlqtizQdO-"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class VLMConfig:\n",
        "    vit_hidden_dim: int = 768\n",
        "    vit_inter_dim: int = 4 * vit_hidden_dim\n",
        "    vit_patch_size: int = 16\n",
        "    vit_img_size: int = 512\n",
        "    vit_n_heads: int = 12\n",
        "    vit_dropout: float = 0.0\n",
        "    vit_n_blocks: int = 12\n",
        "    vit_ln_eps: float = 1e-6\n",
        "    vit_cls_flag: bool = False\n",
        "    vit_model_type: str = 'google/siglip2-base-patch16-512'\n",
        "\n",
        "    lm_hidden_dim: int = 960\n",
        "    lm_inter_dim: int = 2560\n",
        "    lm_rms_eps: float = 1e-5\n",
        "    lm_re_base: int = 100000\n",
        "    lm_max_position_embeddings: int = 8192\n",
        "    lm_base_vocab_size: int = 49152\n",
        "    extra_token_amount: int = 66  # Number of extra tokens for the VLM (image start, image end, image token)\n",
        "    lm_vocab_size: int = lm_base_vocab_size + extra_token_amount # Not a great way to do this, but it works for now (vlm_extra_tokens cannot be a dict, since this is mutable, and a Field has no len() function)\n",
        "    lm_n_heads: int = 15\n",
        "    lm_n_kv_heads: int = 5\n",
        "    lm_dropout: float = 0.0\n",
        "    lm_n_blocks: int = 32\n",
        "    lm_attn_scaling: float = 1.0\n",
        "    lm_max_length: int = 256\n",
        "    lm_use_tokens: bool = False # Decide if the LM expects tokens or embeddings as input (if using as a backbone for the VLM, set to False)\n",
        "    lm_tie_weights: bool = True # Decide if you want to tie the LM Head weight to the token embedding weights\n",
        "    lm_model_type: str = 'HuggingFaceTB/SmolLM2-135M'\n",
        "    lm_tokenizer: str = 'HuggingFaceTB/SmolLM2-360M-Instruct'\n",
        "    lm_chat_template: str = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
        "\n",
        "    mp_pixel_shuffle_factor: int = 4\n",
        "    mp_image_token_length: int = 64\n",
        "\n",
        "    max_img_size: int = 512\n",
        "    resize_to_max_side_len: bool = False\n",
        "\n",
        "    vlm_extra_tokens: dict[str, str] = field(default_factory=lambda: {\"image_token\": \"<|image|>\", \"global_image_token\": \"<|global_image|>\",\n",
        "      \"r1c1\": \"<row_1_col_1>\", \"r1c2\": \"<row_1_col_2>\", \"r1c3\": \"<row_1_col_3>\", \"r1c4\": \"<row_1_col_4>\", \"r1c5\": \"<row_1_col_5>\", \"r1c6\": \"<row_1_col_6>\", \"r1c7\": \"<row_1_col_7>\", \"r1c8\": \"<row_1_col_8>\",\n",
        "      \"r2c1\": \"<row_2_col_1>\", \"r2c2\": \"<row_2_col_2>\", \"r2c3\": \"<row_2_col_3>\", \"r2c4\": \"<row_2_col_4>\", \"r2c5\": \"<row_2_col_5>\", \"r2c6\": \"<row_2_col_6>\", \"r2c7\": \"<row_2_col_7>\", \"r2c8\": \"<row_2_col_8>\",\n",
        "      \"r3c1\": \"<row_3_col_1>\", \"r3c2\": \"<row_3_col_2>\", \"r3c3\": \"<row_3_col_3>\", \"r3c4\": \"<row_3_col_4>\", \"r3c5\": \"<row_3_col_5>\", \"r3c6\": \"<row_3_col_6>\", \"r3c7\": \"<row_3_col_7>\", \"r3c8\": \"<row_3_col_8>\",\n",
        "      \"r4c1\": \"<row_4_col_1>\", \"r4c2\": \"<row_4_col_2>\", \"r4c3\": \"<row_4_col_3>\", \"r4c4\": \"<row_4_col_4>\", \"r4c5\": \"<row_4_col_5>\", \"r4c6\": \"<row_4_col_6>\", \"r4c7\": \"<row_4_col_7>\", \"r4c8\": \"<row_4_col_8>\",\n",
        "      \"r5c1\": \"<row_5_col_1>\", \"r5c2\": \"<row_5_col_2>\", \"r5c3\": \"<row_5_col_3>\", \"r5c4\": \"<row_5_col_4>\", \"r5c5\": \"<row_5_col_5>\", \"r5c6\": \"<row_5_col_6>\", \"r5c7\": \"<row_5_col_7>\", \"r5c8\": \"<row_5_col_8>\",\n",
        "      \"r6c1\": \"<row_6_col_1>\", \"r6c2\": \"<row_6_col_2>\", \"r6c3\": \"<row_6_col_3>\", \"r6c4\": \"<row_6_col_4>\", \"r6c5\": \"<row_6_col_5>\", \"r6c6\": \"<row_6_col_6>\", \"r6c7\": \"<row_6_col_7>\", \"r6c8\": \"<row_6_col_8>\",\n",
        "      \"r7c1\": \"<row_7_col_1>\", \"r7c2\": \"<row_7_col_2>\", \"r7c3\": \"<row_7_col_3>\", \"r7c4\": \"<row_7_col_4>\", \"r7c5\": \"<row_7_col_5>\", \"r7c6\": \"<row_7_col_6>\", \"r7c7\": \"<row_7_col_7>\", \"r7c8\": \"<row_7_col_8>\",\n",
        "      \"r8c1\": \"<row_8_col_1>\", \"r8c2\": \"<row_8_col_2>\", \"r8c3\": \"<row_8_col_3>\", \"r8c4\": \"<row_8_col_4>\", \"r8c5\": \"<row_8_col_5>\", \"r8c6\": \"<row_8_col_6>\", \"r8c7\": \"<row_8_col_7>\", \"r8c8\": \"<row_8_col_8>\"})\n",
        "    vlm_load_backbone_weights: bool = True\n",
        "    vlm_checkpoint_path: str = 'checkpoints'\n",
        "    hf_repo_name: str = 'nanoVLM'\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    lr_mp: float = 0.005\n",
        "    lr_vision_backbone: float = 0.0005\n",
        "    lr_language_backbone: float = 0.0005\n",
        "    data_cutoff_idx: int = 128 # Let's only use a small subset at first\n",
        "    val_ratio: float = 0.2\n",
        "    batch_size: int = 1\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    max_grad_norm: float = 1.0\n",
        "    max_training_steps: int = 200\n",
        "    max_images_per_example: int = 2\n",
        "    max_images_per_knapsack: int = 8\n",
        "    max_sample_length: int = 256\n",
        "    compile: bool = False\n",
        "    resume_from_vlm_checkpoint: bool = False # Indicate if the training should be resumed from a checkpoint of the whole VLM or you want to start from scratch\n",
        "    train_dataset_path: str = 'HuggingFaceM4/the_cauldron'\n",
        "    train_dataset_name: tuple[str, ...] = (\"tqa\", ) #All options; (\"ai2d\", \"aokvqa\", \"chart2text\", \"chartqa\", \"clevr\", \"cocoqa\", \"datikz\", \"diagram_image_to_text\", \"docvqa\", \"dvqa\", \"figureqa\", \"finqa\", \"geomverse\", \"hateful_memes\", \"hitab\", \"iam\", \"iconqa\", \"infographic_vqa\", \"intergps\", \"localized_narratives\", \"mapqa\", \"multihiertt\", \"ocrvqa\", \"plotqa\", \"raven\", \"rendered_text\", \"robut_sqa\", \"robut_wikisql\", \"robut_wtq\", \"scienceqa\", \"screen2words\", \"st_vqa\", \"tabmwp\", \"tallyqa\", \"tat_qa\", \"textcaps\", \"textvqa\", \"tqa\", \"vistext\", \"visual7w\", \"visualmrc\", \"vqarad\", \"vqav2\", \"vsr\", \"websight\") # \"clevr_math\", \"okvqa\", \"spot_the_diff\", \"nlvr2\", \"mimic_cgd\","
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KmFQwKGcSLr_",
      "metadata": {
        "id": "KmFQwKGcSLr_"
      },
      "source": [
        "### Lets run the training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9MlFpXQFSNdx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9MlFpXQFSNdx",
        "outputId": "32e6645d-59a2-4336-b718-29af1bf562b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resize to max side len: False\n",
            "Loading dataset: tqa\n",
            "Warming up dataloaders...\n",
            "Warmup complete.\n",
            "Loading from backbone weights\n",
            "Successfully loaded google/siglip2-base-patch16-512 weights from safetensors. Model has 86,433,024 parameters.\n",
            "Extending token embeddings from torch.Size([49152, 576]) to torch.Size([49218, 576])\n",
            "Initialized 66 new token embeddings\n",
            "Successfully loaded HuggingFaceTB/SmolLM2-135M weights from safetensors. Model has 134,553,024 parameters.\n",
            "nanoVLM initialized with 228,063,936 parameters\n",
            "Training summary: 105 samples, 105 batches/epoch, batch size 1\n",
            "Starting training loop\n",
            "\n",
            "Step: 0, Loss: 3.9663, Val Loss: 0.9367, Tokens/s: 567.33\n",
            "\n",
            "Step: 20, Loss: 7.2900, Val Loss: 1.9322, Tokens/s: 1010.36\n",
            "Epoch 1 | Train Loss: 1.7018 | Val Loss: 1.9322 | Time: 10.49s | T/s: 557.53\n",
            "Starting training loop\n",
            "\n",
            "Step: 40, Loss: 2.4372, Val Loss: 0.9726, Tokens/s: 553.51\n",
            "Epoch 2 | Train Loss: 1.0362 | Val Loss: 0.9726 | Time: 8.97s | T/s: 652.43\n",
            "Starting training loop\n",
            "\n",
            "Step: 60, Loss: 1.6018, Val Loss: 0.3921, Tokens/s: 754.85\n",
            "\n",
            "Step: 80, Loss: 0.7570, Val Loss: 0.1605, Tokens/s: 1361.02\n",
            "Epoch 3 | Train Loss: 0.2558 | Val Loss: 0.1605 | Time: 10.22s | T/s: 572.37\n",
            "Starting training loop\n",
            "\n",
            "Step: 100, Loss: 0.7570, Val Loss: 0.1350, Tokens/s: 769.04\n",
            "Epoch 4 | Train Loss: 0.1420 | Val Loss: 0.1350 | Time: 9.46s | T/s: 618.17\n",
            "Starting training loop\n",
            "\n",
            "Step: 120, Loss: 0.3959, Val Loss: 0.0734, Tokens/s: 1051.38\n",
            "\n",
            "Step: 140, Loss: 0.3401, Val Loss: 0.0876, Tokens/s: 1324.88\n",
            "Epoch 5 | Train Loss: 0.0904 | Val Loss: 0.0876 | Time: 10.94s | T/s: 534.77\n",
            "Starting training loop\n",
            "\n",
            "Step: 160, Loss: 0.2351, Val Loss: 0.0875, Tokens/s: 540.90\n",
            "Epoch 6 | Train Loss: 0.0833 | Val Loss: 0.0875 | Time: 8.94s | T/s: 654.34\n",
            "Starting training loop\n",
            "\n",
            "Step: 180, Loss: 0.2758, Val Loss: 0.0769, Tokens/s: 998.69\n",
            "\n",
            "Step: 200, Loss: 0.2618, Val Loss: 0.0727, Tokens/s: 1026.54\n",
            "Epoch 7 | Train Loss: 0.0771 | Val Loss: 0.0727 | Time: 10.65s | T/s: 549.46\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HfHubHTTPError",
          "evalue": "(Request ID: Root=1-68f61866-126228ff069090802bc80b66;c9149efc-951a-406b-a441-45fc6803d67e)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"YOUR-HF-USERNAME\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-196005043.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvlm_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVLMConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvlm_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3901988680.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_cfg, vlm_cfg)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvlm_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvlm_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mtotal_training_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/nanoVLM/models/vision_language_model.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, repo_id, private)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Create repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mrepo_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepo_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Created repo: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3777\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mRepoUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.endpoint}/{repo_type}/{repo_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3778\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3779\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3780\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3781\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3765\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3766\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3767\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3768\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexist_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    469\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nMake sure your token has the correct permissions.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: Root=1-68f61866-126228ff069090802bc80b66;c9149efc-951a-406b-a441-45fc6803d67e)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"YOUR-HF-USERNAME\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions."
          ]
        }
      ],
      "source": [
        "vlm_cfg = VLMConfig()\n",
        "train_cfg = TrainConfig()\n",
        "train(train_cfg, vlm_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d938dd",
      "metadata": {
        "id": "78d938dd"
      },
      "source": [
        "As you can see the model trains, so feel free to play around with the architecture or data! Let us know what you build with it!\n",
        "\n",
        "PS: If you want to test the model, check out generate.py to see how to do inference with it"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nanovlm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed4047c923264b84b287185cb20e1be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_b3fb8d676b4f41c7bebe86404bf1de96"
          }
        },
        "20191db82876455caa2f602bc1f203ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96f0cd89f26f42efa190035c3a5c4099",
            "placeholder": "​",
            "style": "IPY_MODEL_0858278bb548452384b6522759dd0728",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "0d8b5fe85096456b805c115b18d51fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7cd1449fa16c462e96fa8fdbc4c94c49",
            "placeholder": "​",
            "style": "IPY_MODEL_b26e5515e862490ea9b2a4a35015a86d",
            "value": ""
          }
        },
        "cb61f8ac0ddb4411a733b95c04609c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_4ed4d2e22908481ab7db4f1e14ded317",
            "style": "IPY_MODEL_41198e8b25f54b93ae0dadb07abb5130",
            "value": true
          }
        },
        "a2f93091173d421dbb3e752c21270b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_92ffc2d0fbc14caaaee083607901b5e6",
            "style": "IPY_MODEL_6c4868f079dd4960a41a06b695d0fbcc",
            "tooltip": ""
          }
        },
        "a74ca7bb1a5444e7982deeffc1e2ab12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ec722d1e43e473f8167df2eed9f66ed",
            "placeholder": "​",
            "style": "IPY_MODEL_aee7f4640b7b4ef9a657ca83c542e4fc",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b3fb8d676b4f41c7bebe86404bf1de96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "96f0cd89f26f42efa190035c3a5c4099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0858278bb548452384b6522759dd0728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cd1449fa16c462e96fa8fdbc4c94c49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b26e5515e862490ea9b2a4a35015a86d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ed4d2e22908481ab7db4f1e14ded317": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41198e8b25f54b93ae0dadb07abb5130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92ffc2d0fbc14caaaee083607901b5e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c4868f079dd4960a41a06b695d0fbcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "2ec722d1e43e473f8167df2eed9f66ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aee7f4640b7b4ef9a657ca83c542e4fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c90bc10cb3a6462f98d2a141e8685a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e504b4d2ca24bb497905a7c4c23145d",
            "placeholder": "​",
            "style": "IPY_MODEL_568e8646239d4b3693b88f427ee1df8a",
            "value": "Connecting..."
          }
        },
        "4e504b4d2ca24bb497905a7c4c23145d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "568e8646239d4b3693b88f427ee1df8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}